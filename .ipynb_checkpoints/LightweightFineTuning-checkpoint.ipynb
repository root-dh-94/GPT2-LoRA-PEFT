{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "* PEFT technique: LoRA + quantization\n",
    "* Model: GP2\n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## env imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftConfig, PeftModel, AutoPeftModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"gpt2\"\n",
    "split = [\"train\", \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38669b00",
   "metadata": {},
   "source": [
    "## Load and visualize imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {key: data for key, data in zip(split, dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0b86d",
   "metadata": {},
   "source": [
    "## Load and initalize Tokenizer + tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342dcd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0d208-9e51-4e31-8ea1-be2f2828f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dict()\n",
    "for spl in split:\n",
    "    tokenized_dataset[spl] = dataset[spl].map(lambda x: tokenizer(x[\"text\"], \n",
    "                                                                  truncation=True,\n",
    "                                                                  padding=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bac6a1-a48c-493d-8b2a-c15c25468a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342db38-ab20-4bb9-a952-21c5422af7b4",
   "metadata": {},
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede5626-e315-41b0-b8ca-1f663cbf004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(data):\n",
    "    pred, label = data\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    return {\"accuracy\": (pred == label).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1870c-c668-47e8-892f-cefea7886b66",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd599f-e7de-43bb-b842-16a0cbe15bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_noPEFT = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "                                                  num_labels=2,\n",
    "                                                  id2label={0: \"Negative\", 1: \"Positive\"},\n",
    "                                                  label2id={\"Negative\": 0, \"Postive\": 1})\n",
    "model_noPEFT.config.pad_token_id = model_noPEFT.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56329d9-89b3-4116-9f54-d39a9a08fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_noPEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de4886-2c52-4bd8-a087-b7ffc7efaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trainable parameters\n",
    "trainable_params_noPEFT = 0\n",
    "count = 0\n",
    "for param in model_noPEFT.parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params_noPEFT += param.numel()\n",
    "print(f\"Number of trainable parameters for base foundation model: {trainable_params_noPEFT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdc131-ad6b-47ea-8d75-da825acf682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./output/noPEFT\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20481ede-9f74-4e62-a614-fa1e40a02dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_noPEFT = Trainer(\n",
    "    model=model_noPEFT,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99475696-a00b-40aa-b1c6-5bbb2fba7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_noPEFT.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_noPEFT.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "creating a PEFT model from loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PEFT = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "                                                  num_labels=2,\n",
    "                                                  id2label={0: \"Negative\", 1: \"Positive\"},\n",
    "                                                  label2id={\"Negative\": 0, \"Postive\": 1})\n",
    "model_PEFT.config.pad_token_id = model_PEFT.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    modules_to_save=[\"score\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model_PEFT, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7794c-1eea-4800-a57d-497331811cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_trainable_params = 0\n",
    "total_params = 0\n",
    "for param in lora_model.parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        lora_trainable_params += param.numel()\n",
    "print(f\"Number of trainable parameters in PEFT LoRA model: {lora_trainable_params}\")\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_PEFT = TrainingArguments(\n",
    "    output_dir = \"./output/isPEFT\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_PEFT = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args_PEFT,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c31d69-859f-4fa6-8ef3-55ac57ce1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_PEFT.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\"output/isPEFT/checkpoint-25000\", id2label={0: \"Negative\", 1: \"Positive\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_PEFT = TrainingArguments(\n",
    "    output_dir = \"./output/isPEFT\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_inference = Trainer(\n",
    "    model=inference_model,\n",
    "    args=training_args_PEFT,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_inference.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
